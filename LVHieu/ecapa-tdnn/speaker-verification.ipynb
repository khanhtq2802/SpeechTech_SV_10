{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["n_WBgkP9nTNE","mooxmEzSKuq9","EdFp94o8OaAc","X_5twB6rK5VE","bFxejbCWqMkC","b6nuSg1x1GoI","ky034tZlsAGE","8pTTeUsA1RF-","hjxZMahcwPcO"],"machine_shape":"hm","name":"ECAPA_train.ipynb","provenance":[{"file_id":"15G9nUVBOjuPxID1eMsIZL0zu8X9wlM0Q","timestamp":1643923542419}]},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11737162,"sourceType":"datasetVersion","datasetId":7368375},{"sourceId":11837407,"sourceType":"datasetVersion","datasetId":7437064},{"sourceId":11837911,"sourceType":"datasetVersion","datasetId":7437452}],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Clone repo","metadata":{}},{"cell_type":"code","source":"%%capture\n!git clone https://github.com/taoruijie/ecapa-tdnn.git\n%cd ecapa-tdnn\n\n# Cài đặt các thư viện cần thiết\n%pip install -r requirements.txt\n%pip install librosa==0.9.2  \n\n# Tạo folder lưu mô hình\n!mkdir -p exps/\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:45:35.688230Z","iopub.execute_input":"2025-05-18T03:45:35.688500Z","iopub.status.idle":"2025-05-18T03:45:46.425525Z","shell.execute_reply.started":"2025-05-18T03:45:35.688480Z","shell.execute_reply":"2025-05-18T03:45:46.424346Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"##  Ghi đè dataLoader.py để phù hợp với Vietnam-Celeb và bỏ các hàm thêm nhiễu, thêm tiếng vang","metadata":{}},{"cell_type":"code","source":"code = '''\nimport os\nimport torch\nimport torchaudio\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\nimport random\n\nclass trainDataset(Dataset):\n    def __init__(self, data_list_path, data_path, max_length=16000*5, noise_level=0.005, add_noise=True):\n        self.data_path = data_path\n        self.max_length = max_length\n        self.data_list = []\n        self.noise_level = noise_level\n        self.add_noise = add_noise\n\n        with open(data_list_path, 'r') as f:\n            lines = f.readlines()\n        \n        self.speakers = sorted(list(set([line.strip().split('\\t')[0] for line in lines])))\n        self.spk2id = {spk: idx for idx, spk in enumerate(self.speakers)}\n\n        for line in lines:\n            spk, utt = line.strip().split('\\t')\n            full_path = os.path.join(data_path, spk, utt)\n            if os.path.exists(full_path):\n                self.data_list.append([os.path.join(spk, utt), self.spk2id[spk]])\n            else:\n                print(f\"[Warning] Missing file: {full_path}\")\n\n    def __len__(self):\n        return len(self.data_list)\n\n    def __getitem__(self, index):\n        utt, label = self.data_list[index]\n        audio_path = os.path.join(self.data_path, utt)\n\n        try:\n            audio, _ = torchaudio.load(audio_path)\n            audio = audio.squeeze(0)  # assume mono\n        except Exception as e:\n            print(f\"[Error] Failed to load {audio_path}: {e}\")\n            return self.__getitem__((index + 1) % len(self.data_list))\n\n        # Pad or trim to max_length\n        if audio.shape[0] < self.max_length:\n            pad_len = self.max_length - audio.shape[0]\n            audio = F.pad(audio, (0, pad_len))\n        else:\n            audio = audio[:self.max_length]\n\n        # Thêm white noise augmentation nếu bật\n        if self.add_noise:\n            noise = torch.randn_like(audio) * self.noise_level\n            audio = audio + noise\n            # Clamp để tránh vượt ngoài [-1,1]\n            audio = torch.clamp(audio, -1.0, 1.0)\n\n        return audio, label\n\n\n'''\n\nwith open(\"dataLoader.py\", \"w\") as f:\n    f.write(code)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:45:46.426479Z","iopub.execute_input":"2025-05-18T03:45:46.426715Z","iopub.status.idle":"2025-05-18T03:45:46.431749Z","shell.execute_reply.started":"2025-05-18T03:45:46.426689Z","shell.execute_reply":"2025-05-18T03:45:46.430756Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"## Ghi đè các file train, loss","metadata":{}},{"cell_type":"code","source":"train_code = '''\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport os\nimport argparse\nfrom dataLoader import trainDataset\nfrom ECAPAModel import ECAPA_TDNN\nfrom torch.utils.data import DataLoader\nfrom lossFunction import AAMsoftmax\nfrom torch.optim.lr_scheduler import CosineAnnealingLR\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--train_list', type=str, required=True)\nparser.add_argument('--eval_list', type=str, default='')\nparser.add_argument('--data_path', type=str, required=True)\nparser.add_argument('--save_path', type=str, required=True)\nparser.add_argument('--initial_model', type=str, default='')\nparser.add_argument('--batch_size', type=int, default=64)\nparser.add_argument('--epochs', type=int, default=15)\nparser.add_argument('--noise_level', type=float, default=0.005, help=\"Mức độ noise trắng thêm vào\")\nargs = parser.parse_args()\n\nif not os.path.exists(args.save_path):\n    os.makedirs(args.save_path)\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel = ECAPA_TDNN(C=1024).to(device)\naamsoftmax_layer = AAMsoftmax(n_class=5000).to(device)\ncriterion = nn.CrossEntropyLoss().to(device)\noptimizer = optim.SGD(list(model.parameters()) + list(aamsoftmax_layer.parameters()),\n                      lr=0.01, momentum=0.9, weight_decay=1e-4)\nscheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)\n\nif args.initial_model != \"\":\n    print(\"Loading initial model:\", args.initial_model)\n    checkpoint = torch.load(args.initial_model, map_location=device)\n    if 'state_dict' in checkpoint:\n        state_dict = checkpoint['state_dict']\n    elif 'model' in checkpoint:\n        state_dict = checkpoint['model']\n    else:\n        state_dict = checkpoint\n\n    new_state_dict = {}\n    for k, v in state_dict.items():\n        if k.startswith(\"speaker_encoder.\"):\n            new_key = k.replace(\"speaker_encoder.\", \"\")\n            new_state_dict[new_key] = v\n        else:\n            new_state_dict[k] = v\n\n    missing, unexpected = model.load_state_dict(new_state_dict, strict=False)\n    if missing:\n        print(\"Missing keys:\", missing)\n    if unexpected:\n        print(\"Unexpected keys:\", unexpected)\n\ntrain_dataset = trainDataset(args.train_list, args.data_path, noise_level=args.noise_level)\ntrain_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=2)\n\nprint(\"Start Training\")\nfor epoch in range(1, args.epochs + 1):\n    model.train()\n    aamsoftmax_layer.train()\n    running_loss = 0.0\n    for idx, (audios, labels) in enumerate(train_loader):\n        audios = audios.to(device)\n        labels = labels.to(device)\n\n        optimizer.zero_grad()\n        embeddings = model(audios, aug=True)\n        logits = aamsoftmax_layer(embeddings, labels)\n        loss = criterion(logits, labels)\n\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item()\n        if idx % 100 == 0:\n            avg_loss = running_loss / (idx + 1)\n            print(f\"Epoch {epoch} [{idx}/{len(train_loader)}] Avg Loss: {avg_loss:.4f}\")\n\n    scheduler.step()\n    torch.save({'model': model.state_dict(),\n                'aamsoftmax': aamsoftmax_layer.state_dict()}, os.path.join(args.save_path, f'model_{epoch}.pt'))\n\ntorch.save({'model': model.state_dict(),\n            'aamsoftmax': aamsoftmax_layer.state_dict()}, os.path.join(args.save_path, 'final.model'))\n\n\n'''\n\nwith open(\"trainECAPAModel.py\", \"w\") as f:\n    f.write(train_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:45:46.433031Z","iopub.execute_input":"2025-05-18T03:45:46.433293Z","iopub.status.idle":"2025-05-18T03:45:46.450375Z","shell.execute_reply.started":"2025-05-18T03:45:46.433273Z","shell.execute_reply":"2025-05-18T03:45:46.449729Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"loss_code = '''\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass AAMsoftmax(nn.Module):\n    def __init__(self, n_class, m=0.2, s=30):\n        super(AAMsoftmax, self).__init__()\n        self.n_class = n_class\n        self.m = m\n        self.s = s\n        self.weight = nn.Parameter(torch.randn(n_class, 192))\n        nn.init.xavier_uniform_(self.weight)\n\n    def forward(self, x, label):\n        x_norm = F.normalize(x, dim=1)\n        w_norm = F.normalize(self.weight, dim=1)\n        cosine = torch.matmul(x_norm, w_norm.t())\n        phi = cosine - self.m\n\n        one_hot = torch.zeros_like(cosine)\n        one_hot.scatter_(1, label.view(-1, 1), 1)\n        output = (one_hot * phi) + ((1.0 - one_hot) * cosine)\n        output *= self.s\n        return output\n\n'''\nwith open(\"lossFunction.py\", \"w\") as f:\n    f.write(loss_code)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T03:45:46.451269Z","iopub.execute_input":"2025-05-18T03:45:46.451470Z","iopub.status.idle":"2025-05-18T03:45:46.469704Z","shell.execute_reply.started":"2025-05-18T03:45:46.451453Z","shell.execute_reply":"2025-05-18T03:45:46.468943Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"## Train mô hình với tập train từ Vietnam-Celeb","metadata":{}},{"cell_type":"code","source":"!python trainECAPAModel.py \\\n  --initial_model /kaggle/working/ecapa-tdnn/exps/pretrain.model \\\n  --train_list /kaggle/input/traindatasv/traindata.txt \\\n  --save_path exps/vietnamceleb \\\n  --data_path /kaggle/input/d/vnhiulv/data-sv/data\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T15:30:41.270447Z","iopub.execute_input":"2025-05-17T15:30:41.270661Z"}},"outputs":[],"execution_count":null}]}