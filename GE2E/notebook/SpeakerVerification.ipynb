{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXFVEfX3qyxt",
        "outputId": "8fed65f4-6b84-4cb3-93d3-765da55c6d33"
      },
      "outputs": [],
      "source": [
        "# imports go here\n",
        "import os\n",
        "import pickle\n",
        "import numpy as np\n",
        "import librosa\n",
        "import random\n",
        "import soundfile as sf\n",
        "import time\n",
        "import re\n",
        "import datetime\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.optim import AdamW\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset, RandomSampler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Global variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H2NQ96-qq-BG"
      },
      "outputs": [],
      "source": [
        "TIMIT_ROOT = \"/kaggle/working/data\"\n",
        "LOGMEL_ROOT = \"/kaggle/working/data/logmels/\"\n",
        "CHECKPT_DIR = \"/kaggle/working/SV-checkpts/checkpoint5\"\n",
        "\n",
        "num_frames = 180                # Number of frames after preprocessing\n",
        "hop = 0.01                      # Hop length in s\n",
        "window = 0.025                  # Window size in s\n",
        "n_fft = 512                     # Length of windowed signal after padding\n",
        "sr = 16000                      # Sampling rate\n",
        "win_length = int(window * sr)   # Window length\n",
        "hop_length = int(hop * sr)      # Hop length\n",
        "n_mels = 40                     # Number of Mel bands\n",
        "epsilon = 1e-8                  # Small amount to add to avoid taking log of 0\n",
        "\n",
        "n_hidden = 768                  # Dimensionality of LSTM outputs\n",
        "n_projection = 256              # Dimensionality after projection\n",
        "num_layers = 3                  # Number of LSTM layers\n",
        "n_speakers = 6                  # Number of speakers per batch\n",
        "n_utterances_per_speaker = 10   # Number of utterances per speaker each batch\n",
        "\n",
        "BATCH_SIZE = 16                 # Batch size\n",
        "NUM_EPOCHS = 5                 # Number of epochs\n",
        "\n",
        "force_restart_training = False  # Force training to restart from epoch 0\n",
        "save = True                     # Whether to save model parameters\n",
        "load_opts = True                # Load optimizer states along with model param values\n",
        "halve_after_every = 12          # Number of epochs after which to halve learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fkb9ODbKrCo2"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2V67lDWbrCE7"
      },
      "outputs": [],
      "source": [
        "def get_spectrograms_for_file(file_path):\n",
        "  \"\"\"\n",
        "  Returns the log mel specrogram's first and last n_frames frames for each \"portion\". \n",
        "  Implementation is based on \n",
        "  \"\"\"\n",
        "  min_length = (num_frames*hop + window)*sr\n",
        "  # Load the audio\n",
        "  y, _ = librosa.load(file_path, sr=sr)\n",
        "  # Split the audio into non-silent intervals. \n",
        "  # Reference implementation takes top_db (thresh for silence) to be 30, but librosa\n",
        "  # default is 60.\n",
        "  intervals = librosa.effects.split(y, top_db=30)\n",
        "  extracted = []\n",
        "  for i in range(intervals.shape[0]):\n",
        "    begin = intervals[i][0]\n",
        "    end = intervals[i][1]\n",
        "    if end - begin <= min_length:\n",
        "      continue\n",
        "    # Extract relevant portion of wav\n",
        "    yp = y[begin:end]\n",
        "    # Perform STFT\n",
        "    stft = librosa.stft(y=yp, n_fft=n_fft, win_length=win_length, hop_length=hop_length)\n",
        "    # Squared magnitude of stft - abs necessary because complex\n",
        "    sqmag = np.abs(stft) ** 2\n",
        "    # Get mel basis\n",
        "    M = librosa.filters.mel(sr=sr, n_fft=n_fft, n_mels=n_mels)\n",
        "    # Extract log mel spectrogram\n",
        "    logmel = np.log10(np.dot(M, sqmag) + epsilon)\n",
        "    # Return the first and last n_frames frames\n",
        "    extracted.append(logmel[:, :num_frames])\n",
        "    extracted.append(logmel[:, -num_frames:])\n",
        "  return extracted\n",
        "\n",
        "def get_spectrograms_for_speaker(speaker_dir):\n",
        "  \"\"\"\n",
        "  Given a directory with a speaker's utterances, returns the concatenated list\n",
        "  of extracted log mel features from them *after* converting it into a numpy array.\n",
        "  \"\"\"\n",
        "  extracted = []\n",
        "  for fname in os.listdir(speaker_dir):\n",
        "    if fname.endswith(\".WAV.wav\"):\n",
        "      extracted += get_spectrograms_for_file(os.path.join(speaker_dir, fname))\n",
        "  return np.array(extracted)\n",
        "\n",
        "def save_spectrograms(splits = [\"TRAIN\", \"TEST\"]):\n",
        "  \"\"\"\n",
        "  Call only once. Goes through each speaker dir and saves the generated spectrograms\n",
        "  under LOGMEL_ROOT/{split}\n",
        "  \"\"\"\n",
        "  for split in splits:\n",
        "    split_data_dir = os.path.join(TIMIT_ROOT, split)\n",
        "    split_logmel_dir = os.path.join(LOGMEL_ROOT, split)\n",
        "    for DR in os.listdir(split_data_dir):\n",
        "      DR_dir = os.path.join(split_data_dir, DR)\n",
        "      for speaker in os.listdir(DR_dir):\n",
        "        extracted = get_spectrograms_for_speaker(os.path.join(DR_dir, speaker))\n",
        "        out_file = os.path.join(split_logmel_dir, \"{}.npy\".format(speaker))\n",
        "        np.save(open(out_file, 'wb+'), extracted)\n",
        "\n",
        "def load_data(splits = [\"TRAIN\", \"TEST\"], min_samples=4):\n",
        "  \"\"\"\n",
        "  Loads the dataset -- removes all speakers with < 4 examples.\n",
        "  \"\"\"\n",
        "  data = {}\n",
        "  for split in splits:\n",
        "    part = []\n",
        "    ldir = os.path.join(LOGMEL_ROOT, split)\n",
        "    for fname in os.listdir(ldir):\n",
        "      if not fname.endswith(\".npy\"):\n",
        "        continue\n",
        "      narray = np.load(open(os.path.join(ldir, fname), \"rb\"))\n",
        "      if narray.shape[0] < min_samples:\n",
        "        continue\n",
        "      part.append(narray)\n",
        "    data[split] = part\n",
        "  return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "save_spectrograms()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rrH5w2LMGlbv"
      },
      "outputs": [],
      "source": [
        "class SpeakerVerificationDataset(Dataset):\n",
        "  def __init__(self, logmels, n_speakers=n_speakers, \\\n",
        "    n_samples_per_speaker=n_utterances_per_speaker, total_examples=80000):\n",
        "    \"\"\"\n",
        "    total_examples is the number of examples drawn per epoch\n",
        "    \"\"\"\n",
        "    self.logmels = logmels\n",
        "    self.n_total_speakers = len(self.logmels)\n",
        "    self.n_speakers = n_speakers\n",
        "    self.n_samples_per_speaker = n_samples_per_speaker\n",
        "    self.total_examples = total_examples\n",
        "\n",
        "  def __len__(self):\n",
        "    return self.total_examples\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    \"\"\"\n",
        "    For now we simply ignore idx and return a random sample\n",
        "    \"\"\"\n",
        "    # First, select n different random speakers\n",
        "    # Use the commented code when number of speakers is more\n",
        "    # speakers = np.random.permutation(self.n_total_speakers)[:self.n_speakers]\n",
        "    speakers = []\n",
        "    while len(speakers) < self.n_speakers:\n",
        "      speaker = random.randint(0, self.n_total_speakers-1)\n",
        "      if speaker not in speakers:\n",
        "        speakers.append(speaker)\n",
        "    data = []\n",
        "    for speaker in speakers:\n",
        "      # We may have as low as 8-10 (up to 28) examples per speaker, and we want to choose\n",
        "      # 4-10 of them. A permutation likely avoids the otherwise many tries.\n",
        "      utter_idxs = np.random.permutation(self.logmels[speaker].shape[0])[:self.n_samples_per_speaker]\n",
        "      utterances = torch.from_numpy(self.logmels[speaker][utter_idxs, :, :])\n",
        "      data.append(utterances)\n",
        "    item = torch.stack(data)\n",
        "    # Currently have (speaker, utterance, mel, frames)\n",
        "    # Reorder to (speaker, utterance, frames, mel)\n",
        "    return torch.permute(item, (0,1,3,2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRajZQSYLgjF"
      },
      "outputs": [],
      "source": [
        "data = load_data(min_samples=n_utterances_per_speaker)\n",
        "train_dataset = SpeakerVerificationDataset(data['TRAIN'])\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NfWdopDvNHMt"
      },
      "outputs": [],
      "source": [
        "class SpeakerEmbedder(nn.Module):\n",
        "  \"\"\"\n",
        "  The input to this model is of shape (batch_size*N*M, frames, mel)\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    super(SpeakerEmbedder, self).__init__()\n",
        "    self.LSTMs = nn.LSTM(input_size=n_mels, hidden_size=n_hidden, \\\n",
        "                         num_layers=num_layers, batch_first=True)\n",
        "    self.FC = nn.Linear(n_hidden, n_projection)\n",
        "\n",
        "  def forward(self, x):\n",
        "    LSTMs_out, _ = self.LSTMs(x)\n",
        "    # Current shape is (batch_size*N*M, n_timesteps, n_hidden)\n",
        "    # Need only the last time step\n",
        "    last_out = LSTMs_out[:, LSTMs_out.size(1)-1]\n",
        "    # Now the shape is (batch_size*N*M, n_hidden)\n",
        "    FC_out = self.FC(last_out)\n",
        "    # Normalize each \"row\"\n",
        "    FC_out = FC_out / torch.linalg.norm(FC_out, axis=1).unsqueeze(axis=1)\n",
        "    return FC_out\n",
        "\n",
        "class LossModule(nn.Module):\n",
        "  # Values taken from \n",
        "  def __init__(self):\n",
        "    super(LossModule, self).__init__()\n",
        "    self.w = nn.Parameter(torch.tensor(10.0), requires_grad=True)\n",
        "    self.b = nn.Parameter(torch.tensor(-5.0), requires_grad=True)\n",
        "\n",
        "  def forward(self, embeddings):\n",
        "    # The input should be in the shape (batch_size, N, M, n_projection)\n",
        "    # First get the centroids\n",
        "    centroids = torch.mean(embeddings, dim=2)\n",
        "    N = embeddings.shape[1]\n",
        "    M = embeddings.shape[2]\n",
        "    S = torch.zeros(BATCH_SIZE, N, M, N)\n",
        "    loss = 0\n",
        "  \n",
        "    for b in range(BATCH_SIZE):\n",
        "      for j in range(N):\n",
        "        for i in range(M):\n",
        "          for k in range(N):\n",
        "            if j == k:\n",
        "              # In this case recompute centroid to not include current example\n",
        "              centroid = (M*centroids[b,k] - embeddings[b,j,i]) / (M-1)\n",
        "            else:\n",
        "              centroid = centroids[b,k]\n",
        "            S[b,j,i,k] = self.w*torch.dot(embeddings[b,j,i], centroid) + self.b\n",
        "            if j == k:\n",
        "              loss -= S[b,j,i,k]\n",
        "    expsum = torch.sum(torch.exp(S), axis=-1)\n",
        "    loss += torch.sum(torch.log(expsum))\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IC7G8g5598B8"
      },
      "outputs": [],
      "source": [
        "embedder = SpeakerEmbedder()\n",
        "lossmodule = LossModule()\n",
        "embedder_optimizer = AdamW(embedder.parameters(), lr=1e-3, eps=epsilon)\n",
        "lossmodule_optimizer = AdamW(lossmodule.parameters(), lr=1e-3, eps=epsilon)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbz7mgZRH93P",
        "outputId": "fa5a6097-ad3c-421e-d126-1860826e4d98"
      },
      "outputs": [],
      "source": [
        "if torch.cuda.is_available():\n",
        "  print(\"Using GPU: {}\".format(torch.cuda.get_device_name(0)))\n",
        "  device = torch.device(\"cuda\")\n",
        "  embedder.cuda()\n",
        "  lossmodule.cuda()\n",
        "else:\n",
        "  print(\"No GPUs available, using CPU\")\n",
        "  device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MLM1InsOJnw9"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "  elapsed_rounded = int(round(elapsed))\n",
        "  return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "def get_max_checkpt(checkpt_dir):\n",
        "  max_checkpt = 0\n",
        "  for filename in os.listdir(checkpt_dir):\n",
        "    if re.match(r\"checkpt-embedder-([0-9]+).pt\", filename):\n",
        "      checkpt_num = int(filename.split('.')[-2].split('-')[-1])\n",
        "      if checkpt_num > max_checkpt:\n",
        "        max_checkpt = checkpt_num\n",
        "  return max_checkpt\n",
        "\n",
        "def load_latest_checkpt(checkpt_dir=CHECKPT_DIR):\n",
        "  if force_restart_training:\n",
        "    return 0\n",
        "  mx_checkpt = get_max_checkpt(checkpt_dir)\n",
        "  if mx_checkpt > 0:\n",
        "    embedder_path = os.path.join(checkpt_dir, \"checkpt-embedder-{}.pt\".format(mx_checkpt))\n",
        "    lossmodule_path = os.path.join(checkpt_dir, \"checkpt-lossmodule-{}.pt\".format(mx_checkpt))\n",
        "    embedder_opt_path = os.path.join(checkpt_dir, \"checkpt-eopt-{}.pt\".format(mx_checkpt))\n",
        "    lossmodule_opt_path = os.path.join(checkpt_dir, \"checkpt-lopt-{}.pt\".format(mx_checkpt))\n",
        "    embedder.load_state_dict(torch.load(embedder_path))\n",
        "    lossmodule.load_state_dict(torch.load(lossmodule_path))\n",
        "    if load_opts:\n",
        "      embedder_optimizer.load_state_dict(torch.load(embedder_opt_path))\n",
        "      lossmodule_optimizer.load_state_dict(torch.load(lossmodule_opt_path))\n",
        "  return mx_checkpt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QKHn0jfDEzwL"
      },
      "outputs": [],
      "source": [
        "def train_models():\n",
        "  start_epoch = load_latest_checkpt()\n",
        "  for epoch in range(start_epoch, NUM_EPOCHS):\n",
        "    print(\"============ Epoch {} / {} ============\".format(epoch+1, NUM_EPOCHS))\n",
        "    print(\"Training phase\")\n",
        "    epoch_loss = 0.0\n",
        "    embedder.train()\n",
        "    lossmodule.train()\n",
        "    epoch_start = time.time()\n",
        "    if (epoch+1) % halve_after_every == 0:\n",
        "      for param_group in embedder_optimizer.param_groups:\n",
        "        param_group['lr'] /= 2\n",
        "      for param_group in lossmodule_optimizer.param_groups:\n",
        "        param_group['lr'] /= 2\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch = batch.to(device)\n",
        "      if step % 40 == 0 and step != 0:\n",
        "        elapsed = format_time(time.time() - epoch_start)\n",
        "        print(\"Batch {} of {}. Elapsed {}\".format(step, len(train_dataloader), elapsed))\n",
        "      N = batch.shape[1]\n",
        "      M = batch.shape[2]\n",
        "      embedder_in = batch.reshape(BATCH_SIZE*N*M, batch.shape[3], batch.shape[4])\n",
        "      embedder.zero_grad()\n",
        "      lossmodule.zero_grad()\n",
        "      embeddings = embedder(embedder_in)\n",
        "      embeddings = embeddings.reshape(BATCH_SIZE, N, M, n_projection)\n",
        "      loss = lossmodule(embeddings)\n",
        "      loss.backward()\n",
        "      epoch_loss += loss.detach()\n",
        "      clip_grad_norm_(embedder.parameters(), 3.0)\n",
        "      clip_grad_norm_(lossmodule.parameters(), 1.0)\n",
        "      embedder_optimizer.step()\n",
        "      lossmodule_optimizer.step()\n",
        "    epoch_loss /= len(train_dataloader) * BATCH_SIZE\n",
        "    print(\"Epoch finished. Average training loss: {}\".format(epoch_loss))\n",
        "\n",
        "    if save:\n",
        "      embedder_path = os.path.join(CHECKPT_DIR, \"checkpt-embedder-{}.pt\".format(epoch+1))\n",
        "      lossmodule_path = os.path.join(CHECKPT_DIR, \"checkpt-lossmodule-{}.pt\".format(epoch+1))\n",
        "      embedder_opt_path = os.path.join(CHECKPT_DIR, \"checkpt-eopt-{}.pt\".format(epoch+1))\n",
        "      lossmodule_opt_path = os.path.join(CHECKPT_DIR, \"checkpt-lopt-{}.pt\".format(epoch+1))\n",
        "      torch.save(embedder.state_dict(), embedder_path)\n",
        "      torch.save(lossmodule.state_dict(), lossmodule_path)\n",
        "      torch.save(embedder_optimizer.state_dict(), embedder_opt_path)\n",
        "      torch.save(lossmodule_optimizer.state_dict(), lossmodule_opt_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "hwzhNvKpIbZE"
      },
      "outputs": [],
      "source": [
        "train_models()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load model after train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "embedder = SpeakerEmbedder()\n",
        "lossmodule = LossModule()\n",
        "if torch.cuda.is_available():\n",
        "\tembedder.cuda()\n",
        "\tlossmodule.cuda()\n",
        "embedder_path = os.path.join(CHECKPT_DIR, \"checkpt-embedder-{}.pt\".format(5))\n",
        "embedder.load_state_dict(torch.load(embedder_path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.nn.utils.rnn import pad_sequence\n",
        "def run_utts_through_model(embedder, utts):\n",
        "    utts = torch.stack(utts)\n",
        "    utts = torch.permute(utts, (0,2,1))\n",
        "    device = next(embedder.parameters()).device\n",
        "    utts = utts.to(device)\n",
        "    reps = embedder(utts)\n",
        "    reps_list = [reps[i] for i in range(reps.shape[0])]\n",
        "    return reps_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def similarity(path1, path2):\n",
        "    \"\"\"\n",
        "    Given the path to two audio paths and a threshold, computes\n",
        "    their similarity score and decides whether both depict the same\n",
        "    person.\n",
        "    \"\"\"\n",
        "    utts1 = get_spectrograms_for_file(path1)\n",
        "    utts2 = get_spectrograms_for_file(path2)\n",
        "    nutts1 = len(utts1)\n",
        "    nutts2 = len(utts2)\n",
        "    if nutts1 == 0 or nutts2 == 0:\n",
        "        avg_similarity = 0\n",
        "        return avg_similarity\n",
        "    #print(nutts1)\n",
        "    #print(nutts2)\n",
        "    utts = utts1 + utts2\n",
        "    utts = [torch.from_numpy(utt) for utt in utts]\n",
        "    \n",
        "    reps = run_utts_through_model(embedder, utts)\n",
        "    reps1, reps2 = reps[:nutts1], reps[nutts1:]\n",
        "    avg_similarity = 0\n",
        "    for i in range(nutts1):\n",
        "        for j in range(nutts2):\n",
        "            avg_similarity += torch.dot(reps1[i], reps2[j])\n",
        "    avg_similarity /= nutts1 * nutts2\n",
        "\n",
        "    return avg_similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "# Đọc file CSV\n",
        "results = []\n",
        "\n",
        "with open('/kaggle/input/test-list-public/test_list_private.csv', 'r') as csv_file:\n",
        "    reader = csv.reader(csv_file, delimiter=\" \")  # tách theo dấu cách\n",
        "    for i, row in enumerate(reader):\n",
        "        if len(row) >= 2:\n",
        "            file1, file2 = row[0], row[1]\n",
        "            #print(f\"/kaggle/input/privatetestspech/private-test-data-sv/{file1}\")\n",
        "            result = similarity(f\"/kaggle/input/datatest/wav/{file1}\", f\"/kaggle/input/datatest/wav/{file2}\")  # Gọi hàm processfile\n",
        "            value = result.item() if isinstance(result, torch.Tensor) else result\n",
        "            results.append(f\"{value:.4f}\")\n",
        "            print(f\"dòng{i}: {result:.4f}\")\n",
        "max_value = max(results)\n",
        "max_index = results.index(max_value)\n",
        "print(f\"Max = {max_value}, ở vị trí {max_index}\")\n",
        "# Xử lý từng cặp file\n",
        "\n",
        "\n",
        "    #result = similarity(f\"/kaggle/input/privatetestspech/private-test-data-sv/{file1}\", f\"/kaggle/input/privatetestspech/private-test-data-sv/{file2}\")  # Gọi hàm processfile\n",
        "    #value = result.item() if isinstance(result, torch.Tensor) else result\n",
        "    #results.append(f\"{value:.4f}\")\n",
        "    #print(f\"dòng{i}: {result:.4f}\")\n",
        "\n",
        "# Ghi kết quả vào file TXT\n",
        "with open('/kaggle/working/predictions.txt', 'w') as txt_file: \n",
        "    for line in results: \n",
        "        txt_file.write(line + '\\n')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SpeakerVerification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
